{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: \n",
    "\n",
    "We aim to develop a `Question-Answering (QA) system` that retrieves and processes data from a `Dataverse`, leveraging models from `Models.corp` and an `in-memory Milvus Vector DB` for efficient indexing and retrieval.\n",
    "\n",
    "### Objective:\n",
    "\n",
    "To build an efficient `Retrieval-Augmented Generation (RAG)` system that pulls data from a `Dataverse`, extracts relevant chunks of text, generates embeddings using a pre-trained transformer model, and answers user queries through an LLM. The system will optimize `document chunking, similarity matching, and querying` to provide accurate and context\n",
    "\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Make sure you are on RH VPN (Some links and services below require internal / VPN access.)\n",
    "2. LangChain - https://github.com/langchain-ai/langchain\n",
    "3. List of hosted/managed LLMs (Models.corp) - https://gitlab.cee.redhat.com/models-corp/user-documentation/-/blob/main/README.md\n",
    "4. Granite 3.1 Model (models.corp) - https://granite-3-1-8b-instruct--apicast-production.apps.int.stc.ai.prod.us-east-1.aws.paas.redhat.com/v1\n",
    "    * Model details available at https://gitlab.cee.redhat.com/models-corp/user-documentation/-/blob/main/models/granite-3-1-8b-instruct.md\n",
    "5. In memory vectordb/Milvus - https://python.langchain.com/docs/integrations/vectorstores/milvus/\n",
    "6. Embedding Model (mixedbread-ai/mxbai-embed-large-v1) - https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1PubMedQA_instruction \n",
    "7. Getting access to a dataset in Dataverse (send an email to dataverse-access-request@redhat.com. You can get access to data sets based on your Red Hat role and project needs.)\n",
    "8. Get access to MOSAIC sandbox environment - https://redhat.service-now.com/help?id=sc_cat_item&sys_id=685b8cf987c74a5079f021b2debb353a\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Connection to Dataverse:**:  \n",
    "\n",
    "   Fetches structured and unstructured data from Dataverse for processing.\n",
    "\n",
    "2. **RecursiveCharacterTextSplitter:**:  \n",
    "\n",
    "   Splits documents into smaller chunks for easier processing.\n",
    "\n",
    "3. **Milvus**:  \n",
    "\n",
    "   Stores document embeddings as vectors for efficient retrieval.\n",
    "\n",
    "4. **HuggingFaceEmbeddings**:  \n",
    "\n",
    "   Generates numerical embeddings from text chunks for similarity-based retrieval.\n",
    "\n",
    "5. **RAG**:  \n",
    "\n",
    "   Uses document retrieval and LLMs to generate context-aware responses to queries.\n",
    "\n",
    "### Integration & Security:\n",
    "- **Environment Variables**: \n",
    "  - Use a `.env` file to store and access API keys locally.\n",
    "  - Make sure not to expose or share this file!\n",
    "  \n",
    "- **External APIs**: \n",
    "  - Utilize model and embedding APIs for seamless interaction between the components of the RAG system.\n",
    "\n",
    "### Expected Outcome:\n",
    "- A robust pipeline capable of:\n",
    "  - **Extracting** Data from Dataverse.\n",
    "  - **Generating** high-quality embeddings for efficient retrieval.\n",
    "  - **Querying** with contextually relevant answers using RAG.\n",
    "  - Providing accurate and consistent responses to user queries in a scalable and secure manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "Uncomment the following cell and install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install snowflake-connector-python pandas langchain_community langchain_text_splitters beautifulsoup4 pymilvus langchain_milvus langchain_huggingface huggingface_hub langchain_openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Dependencies\n",
    "\n",
    "This section includes all necessary libraries and modules required for data processing, embedding generation, retrieval, and querying using the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Connection and Fetching Content from Dataverse\n",
    "\n",
    "This script connects to Dataverse(Snowflake) using `conn_params`, retrieves the `CONTENT` column from the `USER_GUIDES` table, and extracts JSON content for processing. \n",
    "\n",
    "**Note:** Load the user and role parameters from environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Snowflake/lib/python3.9/site-packages/snowflake/connector/options.py:108: UserWarning: You have an incompatible version of 'pyarrow' installed (19.0.1), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://auth.redhat.com/auth/realms/EmployeeIDP/protocol/saml?SAMLRequest=lZJfb9owFMW%2FSuQ9EzuQsdYCKgplQ6IdIqHa9uYkF2LVsVNfp4FvP4c%2FUvfQSpPykDjn%2BHfvPXd0d6hU8AYWpdFjEoWMBKBzU0i9H5NtuujdkACd0IVQRsOYHAHJ3WSEolI1nzau1Bt4bQBd4C%2FSyLsfY9JYzY1AiVyLCpC7nCfTxxXvh4wLRLDO48jFUqD0rNK5mlPatm3YDkJj97TPGKPslnpVJ%2FlC3iHqzxm1Nc7kRl0tB9%2FTB4iIsrhDeIUnrC%2FGe6nPI%2FiMkp1FyH%2Bk6bq3%2FpmkJJheu5sZjU0FNgH7JnPYblbnAtBXsC9Ekau8Z0v0k83MIURt2p0SL5Cbqm6cvzX0b3QHBVVmL%2F2slvMxqV9kcbyX24fZYylW7nWe%2Fn42mH0rm6dDNhS%2FtkkyzNrNojXZ91l6k5Pg%2BZpsv0t2idjAUnd5On%2FE%2Bl97LPZPymI%2BiHnEwuEg%2FkOCuc9TauFOzmvRwocdWihK4U61dd%2FUglAV0oeqVuYIsJyv6XX2tMuJnFeFn8h28r8DGNH37svWPfkgPMgomR%2BDhbGVcB%2FnFIXR6UQWvd1JyhuNNeRyJ6HwcSll2pnvwvnddrYBQidn6L%2FbPfkL&RelayState=ver%3A1-hint%3A11624616758862162-ETMsDgAAAZX%2FE%2BxKABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEA%2Fo3Pe11U%2F0cCbfETESEYYAAACQzPzNBYjFeNLK1hBvPsCHTkVreOdKYACWMrVkot%2F7ydvz49%2Fd15ys1bfcJZcvtaAK0tCTXS7ZrkDH6gOPNrmERntjzs6M6Gw0ZI84X3A9Je91LWVL965WAJ6oElQswkqpPYkQh0AZgoVWCeHNT5DMFA71PMPzNRBTG1oFnUjXeCWikMJbLcjXOuquZ2Bk%2FFvQABTuZX4Ko1ajFnrirjWCLrQti9Nifw%3D%3D&SigAlg=http%3A%2F%2Fwww.w3.org%2F2001%2F04%2Fxmldsig-more%23rsa-sha256&Signature=afFhSI5WpNY%2Bziro8UwKcXx%2B93N03ad0wo3L7K%2FQLHDEk5XMT9ajZAS%2Fhb2bNmdh20PpCZviqsEuf2fxgSuDf6q%2BoFaOcodzyiP3GuoDnvKaZokmfFNkbTdSA7S7TD%2BKZegpH0Rhk9TD%2BRsQmBZbNo0mDT99QrzO5%2FtDE9pgBQ5tEaabHcxSkCOQ4nYEdukD2KCt8cfYmRgPAxpmpm8RTl7nBMOrv77LV0CGlBWKvGJzUKUyrlHzi8rIDgLWbSVuwaCPBRXIoFJ8bld1pLSZSueViu7FB5bcITHnB8IvDfOgaGErU4SHLyRmeyKfDznKbcvodsNPBxS0rdxo3AD3Dg%3D%3D to authenticate...\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Snowflake connection parameters\n",
    "conn_params = {\n",
    "    'account': 'GDADCLC-RHSANDBOX',\n",
    "    'user': os.getenv('SF_USER'), \n",
    "    'authenticator': 'externalbrowser', \n",
    "    'warehouse': 'aipoc_group_xs_wh',\n",
    "    'database': 'AIPOC_DB', \n",
    "    'schema': 'MARTS_RHSC_USER_GUIDES', \n",
    "    'role': os.getenv('SF_ROLE'), \n",
    "}\n",
    "\n",
    "def fetch_documents():\n",
    "    \"\"\"Fetch CONTENT and NAME from Snowflake and return as LangChain Document objects.\"\"\"\n",
    "    conn = snowflake.connector.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch document content and name\n",
    "    query = \"SELECT NAME, CONTENT FROM AIPOC_DB.MARTS_RHSC_USER_GUIDES.USER_GUIDES\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns=['NAME', 'CONTENT'])\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    # Convert JSON variant to readable text\n",
    "    def extract_text(content):\n",
    "        try:\n",
    "            parsed_json = json.loads(content)  \n",
    "            return parsed_json.get(\"content\", \"No content available\")  \n",
    "        except json.JSONDecodeError:\n",
    "            return content  \n",
    "\n",
    "    df['CONTENT'] = df['CONTENT'].apply(extract_text)\n",
    "\n",
    "    \n",
    "    documents = [\n",
    "        Document(page_content=text, metadata={\"name\": name}) \n",
    "        for name, text in zip(df['NAME'], df['CONTENT'])\n",
    "    ]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Fetch list of Documents first after reading from snowflake table\n",
    "documents = fetch_documents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking \n",
    "Splits content into smaller chunks using `RecursiveCharacterTextSplitter` from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "\n",
    "chunked_docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Store Embeddings\n",
    "\n",
    "Generates embeddings for text chunks using the `mxbai-embed-large-v1` model from Hugging Face and stores them in an `in-memory Milvus vector DB` for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Snowflake/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/Snowflake/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Successfully stored embeddings in Milvus!\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "# Store in Milvus\n",
    "vectorstore = Milvus.from_documents(  \n",
    "    documents=chunked_docs,\n",
    "    embedding=embeddings,\n",
    "    connection_args={\"uri\": \"./milvus_demo.db\"},  \n",
    "    drop_old=True, \n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    ")\n",
    "\n",
    "print(\"Successfully stored embeddings in Milvus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Models.Corp Credentials\n",
    "\n",
    "Obtain the API key for Models.Corp by following the instructions at - https://gitlab.cee.redhat.com/models-corp/user-documentation and create a `.env` file at the same location where this notebook is present and insert the the line `ACCESS_TOKEN = \"YOUR TOKEN GOES HERE\"` in the `.env` file \n",
    "\n",
    "As we are going to use **Granite-3.1-8b-instruct** details of the LLM can be found here - https://gitlab.cee.redhat.com/models-corp/user-documentation/-/blob/main/models/granite-3-1-8b-instruct.md?ref_type=heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the access token\n",
    "access_token = os.getenv(\"ACCESS_TOKEN\")\n",
    "\n",
    "model_api_url = \"https://granite-3-1-8b-instruct--apicast-production.apps.int.stc.ai.prod.us-east-1.aws.paas.redhat.com/v1\"\n",
    "model = \"/data/granite-3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLM with RAG\n",
    "\n",
    "This function queries a language model using the `Retrieval-Augmented Generation (RAG)` approach. It retrieves relevant text from Milvus, formats it with a structured prompt, and generates fact-based responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=model, api_key=access_token, base_url=model_api_url, temperature=0.1)\n",
    "\n",
    "# Define the prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create the chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "Question :  How to request a new account?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Response :  To request a new account in Red Hat Sales Cloud, follow these steps:\n",
      "\n",
      "1. Choose the 'Accounts' tab.\n",
      "2. Hit the 'Search Account' button.\n",
      "3. Complete the company name field.\n",
      "4. Fill out as much of this form as possible, including the country.\n",
      "5. Hit the 'Search' button.\n",
      "6. If you see the account in the list, click on the name to open the account record.\n",
      "7. If you need to try another name, hit 'Previous'.\n",
      "8. If a new account is still needed, choose 'Notify Data Custodian'. The request will be researched, and either a new account will be created based on your data input, or guidance will be given.\n",
      "\n",
      "This process ensures that the Information Management Team (IMT) can verify the information provided and add more where applicable, ensuring correct customer data and proper placement in the account hierarchy. The IMT uses data from the Dunn & Bradstreet database and the D&B Buydex model to focus on entities with real spending capacity.\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"How to request a new account?\"\n",
    "\n",
    "res = rag_chain.invoke(query)\n",
    "print(\"--------------------------\\n\")\n",
    "print(\"Question : \",query)\n",
    "print(\"\\n--------------------------\\n\")\n",
    "print(\"Response : \",res)\n",
    "print(\"\\n--------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That looks great. The retriever and the granite model both worked well. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try other hosted models on **Models.Corp**. For the list of models on **Models.corp** follow this link - https://gitlab.cee.redhat.com/models-corp/user-documentation/-/tree/main/models?ref_type=heads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
